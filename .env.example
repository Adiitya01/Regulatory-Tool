# LLM Configuration
# LLM_PROVIDER can be 'local' (LM Studio) or 'hf' (Hugging Face)
LLM_PROVIDER=local

# Hugging Face Configuration (Required if LLM_PROVIDER=hf)
# Get your token from: https://huggingface.co/settings/tokens
HF_TOKEN=your_huggingface_token_here

# Optional: Override the model name
# For local: meta-llama-3.1-8b-instruct
# For HF: mistralai/Mistral-7B-Instruct-v0.3 or mistralai/Mixtral-8x7B-Instruct-v0.1
# LLM_MODEL_NAME=meta-llama-3.1-8b-instruct

# Frontend Configuration
VITE_API_URL=https://regulatory-backend-ifwu.onrender.com

# Storage Configuration (For persistent files on Render)
STORAGE_PROVIDER=local  # Change to 'supabase' for cloud storage
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_KEY=your_service_role_key_here
SUPABASE_BUCKET=dhf-reports
